{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0304381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.1.0-cp39-cp39-win_amd64.whl (192.2 MB)\n",
      "     -------------------------------------- 192.2/192.2 MB 2.7 MB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.16.0-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: opencv-python in c:\\users\\91920\\appdata\\roaming\\python\\python39\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (9.2.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91920\\appdata\\roaming\\python\\python39\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2022.7.1)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\91920\\appdata\\roaming\\python\\python39\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91920\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-2.1.0 torchvision-0.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\91920\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision opencv-python pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "078f506e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc45bbda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad113eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566e9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54da779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8041741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca063f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a9d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285db909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dcc4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "337c2a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting deeplake\n",
      "  Downloading deeplake-3.8.1.tar.gz (568 kB)\n",
      "     -------------------------------------- 568.4/568.4 kB 1.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting humbug>=0.3.1\n",
      "  Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
      "Collecting pathos\n",
      "  Downloading pathos-0.3.1-py3-none-any.whl (82 kB)\n",
      "     ---------------------------------------- 82.1/82.1 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: boto3 in c:\\programdata\\anaconda3\\lib\\site-packages (from deeplake) (1.24.28)\n",
      "Requirement already satisfied: lz4 in c:\\programdata\\anaconda3\\lib\\site-packages (from deeplake) (3.1.3)\n",
      "Requirement already satisfied: pyjwt in c:\\programdata\\anaconda3\\lib\\site-packages (from deeplake) (2.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\91920\\appdata\\roaming\\python\\python39\\site-packages (from deeplake) (1.23.5)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from deeplake) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\91920\\appdata\\roaming\\python\\python39\\site-packages (from deeplake) (8.1.6)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from deeplake) (9.2.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from humbug>=0.3.1->deeplake) (2.28.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->deeplake) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->deeplake) (1.27.28)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->deeplake) (0.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91920\\appdata\\roaming\\python\\python39\\site-packages (from click->deeplake) (0.4.6)\n",
      "Collecting pox>=0.3.3\n",
      "  Downloading pox-0.3.3-py3-none-any.whl (29 kB)\n",
      "Collecting multiprocess>=0.70.15\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "     -------------------------------------- 133.3/133.3 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting ppft>=1.7.6.7\n",
      "  Downloading ppft-1.7.6.7-py3-none-any.whl (56 kB)\n",
      "     -------------------------------------- 56.8/56.8 kB 990.4 kB/s eta 0:00:00\n",
      "Collecting dill>=0.3.7\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "     -------------------------------------- 115.3/115.3 kB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.28.0,>=1.27.28->boto3->deeplake) (1.26.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.28.0,>=1.27.28->boto3->deeplake) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->humbug>=0.3.1->deeplake) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->humbug>=0.3.1->deeplake) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->humbug>=0.3.1->deeplake) (3.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.28->boto3->deeplake) (1.16.0)\n",
      "Building wheels for collected packages: deeplake\n",
      "  Building wheel for deeplake (pyproject.toml): started\n",
      "  Building wheel for deeplake (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for deeplake: filename=deeplake-3.8.1-py3-none-any.whl size=685277 sha256=288fad92b633ee20ac805b3a4636c696df8e06f796bbcdbbb6f5d6b32560adec\n",
      "  Stored in directory: c:\\users\\91920\\appdata\\local\\pip\\cache\\wheels\\5a\\f7\\95\\2dd8fdcf24cad6dd3f3e03e622b97849ee349fd2376a99fdd4\n",
      "Successfully built deeplake\n",
      "Installing collected packages: ppft, pox, dill, multiprocess, humbug, pathos, deeplake\n",
      "Successfully installed deeplake-3.8.1 dill-0.3.7 humbug-0.3.2 multiprocess-0.70.15 pathos-0.3.1 pox-0.3.3 ppft-1.7.6.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script activeloop.exe is installed in 'C:\\Users\\91920\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03eec021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/AFW\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/AFW loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91920\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91920\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to C:\\Users\\91920/.cache\\torch\\hub\\checkpoints\\fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 160M/160M [04:11<00:00, 667kB/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '_CustomLoss__init'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2516\\1747138169.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;31m# Create an instance of your custom loss function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m \u001b[0mcustom_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCustomLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;31m# Define an optimizer (e.g., Adam)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2516\\1747138169.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mCustomLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCustomLoss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'super' object has no attribute '_CustomLoss__init'"
     ]
    }
   ],
   "source": [
    "import deeplake\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "\n",
    "# Load the \"AFW\" dataset from Deeplake\n",
    "ds = deeplake.load(\"hub://activeloop/AFW\")\n",
    "\n",
    "# Define a custom dataset class for the Deeplake dataset\n",
    "class CustomFaceDataset(Dataset):\n",
    "    def __init__(self, deeplake_dataset, transform=None):\n",
    "        self.deeplake_dataset = deeplake_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.deeplake_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, target = self.deeplake_dataset[idx]\n",
    "        image = Image.open(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Define transformations, such as resizing and normalizing the images\n",
    "transform = T.Compose([\n",
    "    T.Resize((300, 300)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create the custom dataset\n",
    "custom_dataset = CustomFaceDataset(ds, transform=transform)\n",
    "\n",
    "# Create a data loader\n",
    "batch_size = 4  # Define your batch size\n",
    "num_workers = 4  # Define the number of workers\n",
    "data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model from torchvision\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Modify the model's classifier to match the number of classes in your dataset\n",
    "num_classes = 2  # 1 for face, 1 for background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Set the model in training mode\n",
    "model.train()\n",
    "\n",
    "# Define a custom loss function for your face detection task (e.g., a combination of RPN and box regression loss)\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        rpn_objectness_loss = predictions['loss_objectness']\n",
    "        rpn_box_reg_loss = predictions['loss_rpn_box_reg']\n",
    "        classifier_loss = predictions['loss_classifier']\n",
    "        box_reg_loss = predictions['loss_box_reg']\n",
    "\n",
    "        # Define weight factors for different loss components\n",
    "        weight_rpn_objectness = 1.0\n",
    "        weight_rpn_box_reg = 1.0\n",
    "        weight_classifier = 1.0\n",
    "        weight_box_reg = 1.0\n",
    "\n",
    "        loss_rpn_objectness = rpn_objectness_loss * weight_rpn_objectness\n",
    "        loss_rpn_box_reg = rpn_box_reg_loss * weight_rpn_box_reg\n",
    "        loss_classifier = classifier_loss * weight_classifier\n",
    "        loss_box_reg = box_reg_loss * weight_box_reg\n",
    "\n",
    "        total_loss = loss_rpn_objectness + loss_rpn_box_reg + loss_classifier + loss_box_reg\n",
    "\n",
    "        return total_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87563031",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 22868, 19560, 24264, 24964) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1132\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1133\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2516\\2487466650.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1328\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1292\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1294\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1295\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1143\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1145\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'DataLoader worker (pid(s) {pids_str}) exited unexpectedly'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 22868, 19560, 24264, 24964) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "custom_loss = CustomLoss()\n",
    "\n",
    "# Define an optimizer (e.g., Adam)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define a learning rate scheduler (e.g., StepLR)\n",
    "lr_scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Train the model on your dataset\n",
    "num_epochs = 10  # Define the number of training epochs\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, targets in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = custom_loss(loss_dict, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'face_detection_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcde54b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa052299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#use saved model\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load('face_detection_model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define a list of class labels for face and background\n",
    "class_labels = ['background', 'face']\n",
    "\n",
    "# Load an image for face detection (you can replace 'image_path' with the path to your image)\n",
    "image_path = 'path_to_your_image.jpg'\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Define a transformation for the input image\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Apply the transformation to the image\n",
    "image = transform(image)\n",
    "\n",
    "# Make a prediction using the trained model\n",
    "with torch.no_grad():\n",
    "    prediction = model([image])\n",
    "\n",
    "# Get the predicted bounding boxes, labels, and scores\n",
    "boxes = prediction[0]['boxes']\n",
    "labels = prediction[0]['labels']\n",
    "scores = prediction[0]['scores']\n",
    "\n",
    "# Filter the results to only include face detections\n",
    "face_boxes = boxes[labels == 1]  # 1 corresponds to 'face' class\n",
    "face_scores = scores[labels == 1]\n",
    "\n",
    "# You can now use the 'face_boxes' and 'face_scores' to work with the detected faces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb9eb09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce2354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed948f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58033e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3d4e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d482f074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8561d39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f246e29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d7bbd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571ab73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65094a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f750cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec1d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5051b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30241efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f2545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf18e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d914f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455bec66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1789c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81005540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47f6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe4bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b1a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "# Define paths to the dataset folders (JPG images and PTS annotations)\n",
    "jpg_folder = 'C:/Users/ravis/Downloads/SHAREit/pc/file/AFW/jpg'\n",
    "pts_folder = 'C:/Users/ravis/Downloads/SHAREit/pc/file/AFW/pts'\n",
    "\n",
    "# Create lists of JPG and PTS file paths\n",
    "jpg_files = [os.path.join(jpg_folder, filename) for filename in os.listdir(jpg_folder) if filename.endswith('.jpg')]\n",
    "pts_files = [os.path.join(pts_folder, filename) for filename in os.listdir(pts_folder) if filename.endswith('.pts')]\n",
    "\n",
    "# Print the list of JPG file paths\n",
    "print(\"JPG Files:\")\n",
    "for jpg_file in jpg_files:\n",
    "    print(jpg_file)\n",
    "\n",
    "# Print the list of PTS file paths\n",
    "print(\"\\nPTS Files:\")\n",
    "for pts_file in pts_files:\n",
    "    print(pts_file)\n",
    "\n",
    "# Define a function to parse PTS annotation files and generate bounding box coordinates\n",
    "def generate_bbox_from_pts(pts_file):\n",
    "    with open(pts_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Initialize lists to store facial landmark points\n",
    "    landmarks = []\n",
    "\n",
    "    # Iterate through lines to extract facial landmark points\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            try:\n",
    "                x, y = map(float, parts)\n",
    "                landmarks.append((x, y))\n",
    "            except ValueError:\n",
    "                continue  # Skip lines that cannot be converted to float\n",
    "\n",
    "    if len(landmarks) >= 2:\n",
    "        # Calculate bounding box coordinates\n",
    "        x_values, y_values = zip(*landmarks)\n",
    "        x1, y1 = min(x_values), min(y_values)\n",
    "        x2, y2 = max(x_values), max(y_values)\n",
    "    else:\n",
    "        # If there are not enough valid landmarks, set default values or handle as needed\n",
    "        x1, y1, x2, y2 = 0, 0, 0, 0  # You can adjust these default values\n",
    "\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "# Loop through each JPG file and its corresponding PTS file\n",
    "for jpg_path, pts_path in zip(jpg_files, pts_files):\n",
    "    # Load the JPG image\n",
    "    image = cv2.imread(jpg_path)\n",
    "\n",
    "    # Parse the PTS file to get bounding box coordinates\n",
    "    x1, y1, x2, y2 = generate_bbox_from_pts(pts_path)\n",
    "\n",
    "    # Draw the bounding box on the image\n",
    "    cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "    # Create a segmentation mask (a white rectangle in this example)\n",
    "    segmentation_mask = np.zeros_like(image)  # Initialize an all-black mask\n",
    "    segmentation_mask[int(y1):int(y2), int(x1):int(x2)] = [255, 255, 255]  # Fill the bounding box with white\n",
    "\n",
    "    # Display or save the image with the bounding box and segmentation mask\n",
    "    # Display only a few of the images (e.g., display the first 3 images)\n",
    "    if jpg_path in jpg_files[:3]:  # Change '3' to the number of images you want to display\n",
    "        cv2.imshow('Image with Bounding Box', image)\n",
    "        cv2.imshow('Segmentation Mask', segmentation_mask)\n",
    "        cv2.waitKey(0)\n",
    "# Close any open windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Split your dataset into training, validation, and test sets\n",
    "# You can adjust the split ratios as needed\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "total_samples = len(jpg_files)\n",
    "train_samples = int(train_ratio * total_samples)\n",
    "val_samples = int(val_ratio * total_samples)\n",
    "\n",
    "train_jpg_files = jpg_files[:train_samples]\n",
    "val_jpg_files = jpg_files[train_samples:train_samples + val_samples]\n",
    "test_jpg_files = jpg_files[train_samples + val_samples:]\n",
    "\n",
    "# Define a custom dataset class to load and preprocess the data\n",
    "class CustomDataset(Dataset):\n",
    "    def _init_(self, jpg_files, pts_files, transform=None):\n",
    "        self.jpg_files = jpg_files\n",
    "        self.pts_files = pts_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def _len_(self):\n",
    "        return len(self.jpg_files)\n",
    "\n",
    "    def _getitem_(self, idx):\n",
    "        jpg_path = self.jpg_files[idx]\n",
    "        pts_path = self.pts_files[idx]\n",
    "\n",
    "        # Load the JPG image\n",
    "        image = cv2.imread(jpg_path)\n",
    "\n",
    "        # Generate bounding box coordinates from PTS file\n",
    "        x1, y1, x2, y2 = generate_bbox_from_pts(pts_path)\n",
    "\n",
    "        # Create a segmentation mask (an all-black mask in this example)\n",
    "        segmentation_mask = np.zeros_like(image)\n",
    "\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'bounding_box': [x1, y1, x2, y2],\n",
    "            'segmentation_mask': segmentation_mask\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# Create custom datasets for training, validation, and testing\n",
    "train_dataset = CustomDataset(train_jpg_files, train_jpg_files)\n",
    "val_dataset = CustomDataset(val_jpg_files, val_jpg_files)\n",
    "test_dataset = CustomDataset(test_jpg_files, test_jpg_files)\n",
    "\n",
    "# Create data loaders for training, validation, and testing\n",
    "batch_size = 4  # Adjust the batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define and load pre-trained Faster R-CNN model\n",
    "faster_rcnn_model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "faster_rcnn_model.eval()\n",
    "\n",
    "# Define and load pre-trained Mask R-CNN model\n",
    "mask_rcnn_model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "mask_rcnn_model.eval()\n",
    "\n",
    "# Faster R-CNN Loss Function\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "regression_loss = nn.SmoothL1Loss()\n",
    "\n",
    "# Mask R-CNN Loss Function\n",
    "mask_loss = nn.BCELoss()  # For binary masks, use Binary Cross-Entropy Loss\n",
    "\n",
    "# Combine Faster R-CNN and Mask R-CNN Losses\n",
    "alpha = 1.0  # Weight for Faster R-CNN classification loss\n",
    "beta = 1.0   # Weight for Faster R-CNN regression loss\n",
    "gamma = 1.0  # Weight for Mask R-CNN loss\n",
    "\n",
    "# Define the combined loss\n",
    "def combined_loss(classification_loss, regression_loss, mask_loss, alpha, beta, gamma):\n",
    "    def loss_fn(outputs, targets):\n",
    "        # Extract the outputs of both Faster R-CNN and Mask R-CNN\n",
    "        faster_rcnn_outputs, mask_rcnn_outputs = outputs\n",
    "\n",
    "        # Extract the targets for Faster R-CNN and Mask R-CNN\n",
    "        faster_rcnn_targets, mask_rcnn_targets = targets\n",
    "\n",
    "        # Compute Faster R-CNN losses\n",
    "        faster_rcnn_classification_loss = classification_loss(faster_rcnn_outputs['labels'], faster_rcnn_targets['labels'])\n",
    "        faster_rcnn_regression_loss = regression_loss(faster_rcnn_outputs['boxes'], faster_rcnn_targets['boxes'])\n",
    "\n",
    "        # Compute Mask R-CNN loss\n",
    "        mask_rcnn_loss = mask_loss(mask_rcnn_outputs['masks'], mask_rcnn_targets['masks'])\n",
    "\n",
    "        # Combine the losses\n",
    "        total_loss = alpha * (faster_rcnn_classification_loss + faster_rcnn_regression_loss) + gamma * mask_rcnn_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    return loss_fn\n",
    "\n",
    "# Create the combined loss function\n",
    "combined_losses = combined_loss(classification_loss, regression_loss, mask_loss, alpha, beta, gamma)\n",
    "\n",
    "# Set the learning rate for the optimizer\n",
    "learning_rate = 0.001  # You can adjust the learning rate as needed\n",
    "\n",
    "# Create an optimizer for the Faster R-CNN model\n",
    "optimizer = Adam(faster_rcnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop for Faster R-CNN\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass using the Faster R-CNN model\n",
    "        outputs = faster_rcnn_model(batch['image'])  # Use faster_rcnn_model, not combined_model\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = combined_losses(outputs, batch['targets'])\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print or log training statistics, e.g., loss, accuracy\n",
    "\n",
    "    # Validation and model evaluation\n",
    "        for batch in val_loader:\n",
    "            # Perform validation and evaluation\n",
    "\n",
    "    # Save the trained Faster R-CNN model if needed\n",
    "     torch.save(faster_rcnn_model.state_dict(), 'faster_rcnn_model.pth')\n",
    "\n",
    "# Set up a separate optimizer for the Mask R-CNN model\n",
    "mask_rcnn_optimizer = Adam(mask_rcnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop for Mask R-CNN\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        mask_rcnn_optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass using the Mask R-CNN model\n",
    "        mask_outputs = mask_rcnn_model(batch['image'])\n",
    "\n",
    "        # Calculate the Mask R-CNN loss\n",
    "        mask_loss = mask_loss(mask_outputs, batch['mask_targets'])  # Define 'mask_targets' accordingly\n",
    "\n",
    "        # Backpropagation for Mask R-CNN\n",
    "        mask_loss.backward()\n",
    "\n",
    "        # Update Mask R-CNN model parameters\n",
    "        mask_rcnn_optimizer.step()\n",
    "\n",
    "        # Calculate the average loss for this epoch\n",
    "        average_mask_loss = mask_loss / len(train_loader)\n",
    "\n",
    "        # Print or log the training statistics for this epoch\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] - Mask Loss: {average_mask_loss}')\n",
    "\n",
    "        # Print or log training statistics for Mask R-CNN\n",
    "\n",
    "    # Validation and model evaluation for Mask R-CNN\n",
    "    for batch in val_loader:\n",
    "        # Perform validation and evaluation for Mask R-CNN\n",
    "\n",
    "# Save the trained Mask R-CNN model if needed\n",
    "torch.save(mask_rcnn_model.state_dict(), 'mask_rcnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b449e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d4a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb29f92b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sahill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f98cbf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3835902363.py, line 222)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\91920\\AppData\\Local\\Temp\\ipykernel_2516\\3835902363.py\"\u001b[1;36m, line \u001b[1;32m222\u001b[0m\n\u001b[1;33m    torch.save(faster_rcnn_model.state_dict(), 'faster_rcnn_model.pth')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "# Define paths to the dataset folders (JPG images and PTS annotations)\n",
    "jpg_folder = 'C:/Users/ravis/Downloads/SHAREit/pc/file/AFW/jpg'\n",
    "pts_folder = 'C:/Users/ravis/Downloads/SHAREit/pc/file/AFW/pts'\n",
    "\n",
    "# Create lists of JPG and PTS file paths\n",
    "jpg_files = [os.path.join(jpg_folder, filename) for filename in os.listdir(jpg_folder) if filename.endswith('.jpg')]\n",
    "pts_files = [os.path.join(pts_folder, filename) for filename in os.listdir(pts_folder) if filename.endswith('.pts')]\n",
    "\n",
    "# Print the list of JPG file paths\n",
    "print(\"JPG Files:\")\n",
    "for jpg_file in jpg_files:\n",
    "    print(jpg_file)\n",
    "\n",
    "# Print the list of PTS file paths\n",
    "print(\"\\nPTS Files:\")\n",
    "for pts_file in pts_files:\n",
    "    print(pts_file)\n",
    "\n",
    "# Define a function to parse PTS annotation files and generate bounding box coordinates\n",
    "def generate_bbox_from_pts(pts_file):\n",
    "    with open(pts_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Initialize lists to store facial landmark points\n",
    "    landmarks = []\n",
    "\n",
    "    # Iterate through lines to extract facial landmark points\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            try:\n",
    "                x, y = map(float, parts)\n",
    "                landmarks.append((x, y))\n",
    "            except ValueError:\n",
    "                continue  # Skip lines that cannot be converted to float\n",
    "\n",
    "    if len(landmarks) >= 2:\n",
    "        # Calculate bounding box coordinates\n",
    "        x_values, y_values = zip(*landmarks)\n",
    "        x1, y1 = min(x_values), min(y_values)\n",
    "        x2, y2 = max(x_values), max(y_values)\n",
    "    else:\n",
    "        # If there are not enough valid landmarks, set default values or handle as needed\n",
    "        x1, y1, x2, y2 = 0, 0, 0, 0  # You can adjust these default values\n",
    "\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "# Loop through each JPG file and its corresponding PTS file\n",
    "for jpg_path, pts_path in zip(jpg_files, pts_files):\n",
    "    # Load the JPG image\n",
    "    image = cv2.imread(jpg_path)\n",
    "\n",
    "    # Parse the PTS file to get bounding box coordinates\n",
    "    x1, y1, x2, y2 = generate_bbox_from_pts(pts_path)\n",
    "\n",
    "    # Draw the bounding box on the image\n",
    "    cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "    # Create a segmentation mask (a white rectangle in this example)\n",
    "    segmentation_mask = np.zeros_like(image)  # Initialize an all-black mask\n",
    "    segmentation_mask[int(y1):int(y2), int(x1):int(x2)] = [255, 255, 255]  # Fill the bounding box with white\n",
    "\n",
    "    # Display or save the image with the bounding box and segmentation mask\n",
    "    # Display only a few of the images (e.g., display the first 3 images)\n",
    "    if jpg_path in jpg_files[:3]:  # Change '3' to the number of images you want to display\n",
    "        cv2.imshow('Image with Bounding Box', image)\n",
    "        cv2.imshow('Segmentation Mask', segmentation_mask)\n",
    "        cv2.waitKey(0)\n",
    "# Close any open windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Split your dataset into training, validation, and test sets\n",
    "# You can adjust the split ratios as needed\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "total_samples = len(jpg_files)\n",
    "train_samples = int(train_ratio * total_samples)\n",
    "val_samples = int(val_ratio * total_samples)\n",
    "\n",
    "train_jpg_files = jpg_files[:train_samples]\n",
    "val_jpg_files = jpg_files[train_samples:train_samples + val_samples]\n",
    "test_jpg_files = jpg_files[train_samples + val_samples:]\n",
    "\n",
    "# Define a custom dataset class to load and preprocess the data\n",
    "class CustomDataset(Dataset):\n",
    "    def _init_(self, jpg_files, pts_files, transform=None):\n",
    "        self.jpg_files = jpg_files\n",
    "        self.pts_files = pts_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def _len_(self):\n",
    "        return len(self.jpg_files)\n",
    "\n",
    "    def _getitem_(self, idx):\n",
    "        jpg_path = self.jpg_files[idx]\n",
    "        pts_path = self.pts_files[idx]\n",
    "\n",
    "        # Load the JPG image\n",
    "        image = cv2.imread(jpg_path)\n",
    "\n",
    "        # Generate bounding box coordinates from PTS file\n",
    "        x1, y1, x2, y2 = generate_bbox_from_pts(pts_path)\n",
    "\n",
    "        # Create a segmentation mask (an all-black mask in this example)\n",
    "        segmentation_mask = np.zeros_like(image)\n",
    "\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'bounding_box': [x1, y1, x2, y2],\n",
    "            'segmentation_mask': segmentation_mask\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# Create custom datasets for training, validation, and testing\n",
    "train_dataset = CustomDataset(train_jpg_files, train_jpg_files)\n",
    "val_dataset = CustomDataset(val_jpg_files, val_jpg_files)\n",
    "test_dataset = CustomDataset(test_jpg_files, test_jpg_files)\n",
    "\n",
    "# Create data loaders for training, validation, and testing\n",
    "batch_size = 4  # Adjust the batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define and load pre-trained Faster R-CNN model\n",
    "faster_rcnn_model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "faster_rcnn_model.eval()\n",
    "\n",
    "# Define and load pre-trained Mask R-CNN model\n",
    "mask_rcnn_model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "mask_rcnn_model.eval()\n",
    "\n",
    "# Faster R-CNN Loss Function\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "regression_loss = nn.SmoothL1Loss()\n",
    "\n",
    "# Mask R-CNN Loss Function\n",
    "mask_loss = nn.BCELoss()  # For binary masks, use Binary Cross-Entropy Loss\n",
    "\n",
    "# Combine Faster R-CNN and Mask R-CNN Losses\n",
    "alpha = 1.0  # Weight for Faster R-CNN classification loss\n",
    "beta = 1.0   # Weight for Faster R-CNN regression loss\n",
    "gamma = 1.0  # Weight for Mask R-CNN loss\n",
    "\n",
    "# Define the combined loss\n",
    "def combined_loss(classification_loss, regression_loss, mask_loss, alpha, beta, gamma):\n",
    "    def loss_fn(outputs, targets):\n",
    "        # Extract the outputs of both Faster R-CNN and Mask R-CNN\n",
    "        faster_rcnn_outputs, mask_rcnn_outputs = outputs\n",
    "\n",
    "        # Extract the targets for Faster R-CNN and Mask R-CNN\n",
    "        faster_rcnn_targets, mask_rcnn_targets = targets\n",
    "\n",
    "        # Compute Faster R-CNN losses\n",
    "        faster_rcnn_classification_loss = classification_loss(faster_rcnn_outputs['labels'], faster_rcnn_targets['labels'])\n",
    "        faster_rcnn_regression_loss = regression_loss(faster_rcnn_outputs['boxes'], faster_rcnn_targets['boxes'])\n",
    "\n",
    "        # Compute Mask R-CNN loss\n",
    "        mask_rcnn_loss = mask_loss(mask_rcnn_outputs['masks'], mask_rcnn_targets['masks'])\n",
    "\n",
    "        # Combine the losses\n",
    "        total_loss = alpha * (faster_rcnn_classification_loss + faster_rcnn_regression_loss) + gamma * mask_rcnn_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    return loss_fn\n",
    "\n",
    "# Create the combined loss function\n",
    "combined_losses = combined_loss(classification_loss, regression_loss, mask_loss, alpha, beta, gamma)\n",
    "\n",
    "# Set the learning rate for the optimizer\n",
    "learning_rate = 0.001  # You can adjust the learning rate as needed\n",
    "\n",
    "# Create an optimizer for the Faster R-CNN model\n",
    "optimizer = Adam(faster_rcnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop for Faster R-CNN\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass using the Faster R-CNN model\n",
    "        outputs = faster_rcnn_model(batch['image'])  # Use faster_rcnn_model, not combined_model\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = combined_losses(outputs, batch['targets'])\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print or log training statistics, e.g., loss, accuracy\n",
    "\n",
    "    # Validation and model evaluation\n",
    "        for batch in val_loader:\n",
    "            # Perform validation and evaluation\n",
    "\n",
    "    # Save the trained Faster R-CNN model if needed\n",
    "        torch.save(faster_rcnn_model.state_dict(), 'faster_rcnn_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a62c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debab69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up a separate optimizer for the Mask R-CNN model\n",
    "mask_rcnn_optimizer = Adam(mask_rcnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop for Mask R-CNN\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        mask_rcnn_optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass using the Mask R-CNN model\n",
    "        mask_outputs = mask_rcnn_model(batch['image'])\n",
    "\n",
    "        # Calculate the Mask R-CNN loss\n",
    "        mask_loss = mask_loss(mask_outputs, batch['mask_targets'])  # Define 'mask_targets' accordingly\n",
    "\n",
    "        # Backpropagation for Mask R-CNN\n",
    "        mask_loss.backward()\n",
    "\n",
    "        # Update Mask R-CNN model parameters\n",
    "        mask_rcnn_optimizer.step()\n",
    "\n",
    "        # Calculate the average loss for this epoch\n",
    "        average_mask_loss = mask_loss / len(train_loader)\n",
    "\n",
    "        # Print or log the training statistics for this epoch\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] - Mask Loss: {average_mask_loss}')\n",
    "\n",
    "        # Print or log training statistics for Mask R-CNN\n",
    "\n",
    "    # Validation and model evaluation for Mask R-CNN\n",
    "    for batch in val_loader:\n",
    "        # Perform validation and evaluation for Mask R-CNN\n",
    "\n",
    "# Save the trained Mask R-CNN model if needed\n",
    "torch.save(mask_rcnn_model.state_dict(), 'mask_rcnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb996c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b52c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc73e953",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 110)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m110\u001b[0m\n\u001b[1;33m    pts_path = self.pts_files[idx]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "#trying to correct \n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Define paths to the dataset folders (JPG images and PTS annotations)\n",
    "jpg_folder = 'C:/Users/ravis/Downloads/SHAREit/pc/file/AFW/jpg'\n",
    "pts_folder = 'C:/Users/ravis/Downloads/SHAREit/pc/file/AFW/pts'\n",
    "\n",
    "# Create lists of JPG and PTS file paths\n",
    "jpg_files = [os.path.join(jpg_folder, filename) for filename in os.listdir(jpg_folder) if filename.endswith('.jpg')]\n",
    "pts_files = [os.path.join(pts_folder, filename) for filename in os.listdir(pts_folder) if filename.endswith('.pts')]\n",
    "\n",
    "# Print the list of JPG file paths\n",
    "print(\"JPG Files:\")\n",
    "for jpg_file in jpg_files:\n",
    "    print(jpg_file)\n",
    "\n",
    "# Print the list of PTS file paths\n",
    "print(\"\\nPTS Files:\")\n",
    "for pts_file in pts_files:\n",
    "    print(pts_file)\n",
    "\n",
    "# Define a function to parse PTS annotation files and generate bounding box coordinates\n",
    "def generate_bbox_from_pts(pts_file):\n",
    "    with open(pts_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Initialize lists to store facial landmark points\n",
    "    landmarks = []\n",
    "\n",
    "    # Iterate through lines to extract facial landmark points\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            try:\n",
    "                x, y = map(float, parts)\n",
    "                landmarks.append((x, y))\n",
    "            except ValueError:\n",
    "                continue  # Skip lines that cannot be converted to float\n",
    "\n",
    "    if len(landmarks) >= 2:\n",
    "        # Calculate bounding box coordinates\n",
    "        x_values, y_values = zip(*landmarks)\n",
    "        x1, y1 = min(x_values), min(y_values)\n",
    "        x2, y2 = max(x_values), max(y_values)\n",
    "    else:\n",
    "        # If there are not enough valid landmarks, set default values or handle as needed\n",
    "        x1, y1, x2, y2 = 0, 0, 0, 0  # You can adjust these default values\n",
    "\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "# Loop through each JPG file and its corresponding PTS file\n",
    "for jpg_path, pts_path in zip(jpg_files, pts_files):\n",
    "    # Load the JPG image\n",
    "    image = cv2.imread(jpg_path)\n",
    "\n",
    "    # Parse the PTS file to get bounding box coordinates\n",
    "    x1, y1, x2, y2 = generate_bbox_from_pts(pts_path)\n",
    "\n",
    "    # Draw the bounding box on the image\n",
    "    cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "    # Create a segmentation mask (a white rectangle in this example)\n",
    "    segmentation_mask = np.zeros_like(image)  # Initialize an all-black mask\n",
    "    segmentation_mask[int(y1):int(y2), int(x1):int(x2)] = [255, 255, 255]  # Fill the bounding box with white\n",
    "\n",
    "    # Display or save the image with the bounding box and segmentation mask\n",
    "    # Display only a few of the images (e.g., display the first 3 images)\n",
    "    if jpg_path in jpg_files[:3]:  # Change '3' to the number of images you want to display\n",
    "        cv2.imshow('Image with Bounding Box', image)\n",
    "        cv2.imshow('Segmentation Mask', segmentation_mask)\n",
    "        cv2.waitKey(0)\n",
    "# Close any open windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Split your dataset into training, validation, and test sets\n",
    "# You can adjust the split ratios as needed\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "total_samples = len(jpg_files)\n",
    "train_samples = int(train_ratio * total_samples)\n",
    "val_samples = int(val_ratio * total_samples)\n",
    "\n",
    "train_jpg_files = jpg_files[:train_samples]\n",
    "val_jpg_files = jpg_files[train_samples:train_samples + val_samples]\n",
    "test_jpg_files = jpg_files[train_samples + val_samples:]\n",
    "\n",
    "# Define a custom dataset class to load and preprocess the data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, jpg_files, pts_files, transform=None):\n",
    "        self.jpg_files = jpg_files\n",
    "        self.pts_files = pts_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.jpg_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "               jpg_path = self.jpg_files[idx]\n",
    "        pts_path = self.pts_files[idx]\n",
    "\n",
    "        # Load the JPG image\n",
    "        image = cv2.imread(jpg_path)\n",
    "\n",
    "        # Generate bounding box coordinates from PTS file\n",
    "        x1, y1, x2, y2 = generate_bbox_from_pts(pts_path)\n",
    "\n",
    "        # Create a segmentation mask (an all-black mask in this example)\n",
    "        segmentation_mask = np.zeros_like(image)\n",
    "\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'bounding_box': [x1, y1, x2, y2],\n",
    "            'segmentation_mask': segmentation_mask\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "# Create custom datasets for training, validation, and testing\n",
    "train_dataset = CustomDataset(train_jpg_files, train_jpg_files)\n",
    "val_dataset = CustomDataset(val_jpg_files, val_jpg_files)\n",
    "test_dataset = CustomDataset(test_jpg_files, test_jpg_files)\n",
    "\n",
    "# Create data loaders for training, validation, and testing\n",
    "batch_size = 4  # Adjust the batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define and load pre-trained Faster R-CNN model\n",
    "faster_rcnn_model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "faster_rcnn_model.eval()\n",
    "\n",
    "# Faster R-CNN Loss Function\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "regression_loss = nn.SmoothL1Loss()\n",
    "\n",
    "# Set the learning rate for the optimizer\n",
    "learning_rate = 0.001  # You can adjust the learning rate as needed\n",
    "\n",
    "# Create an optimizer for the Faster R-CNN model\n",
    "optimizer = Adam(faster_rcnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop for Faster R-CNN\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass using the Faster R-CNN model\n",
    "        outputs = faster_rcnn_model(batch['image'])  # Use faster_rcnn_model, not combined_model\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = classification_loss(outputs['labels'], batch['targets']) + regression_loss(outputs['boxes'], batch['targets'])\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print or log training statistics, e.g., loss, accuracy\n",
    "\n",
    "    # Validation and model evaluation\n",
    "    for batch in val_loader:\n",
    "        # Perform validation and evaluation\n",
    "\n",
    "    # Save the trained Faster R-CNN model if needed\n",
    "    torch.save(faster_rcnn_model.state_dict(), 'faster_rcnn_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc1c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a024db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3216f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc1ef53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc4177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e06e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f16f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95270a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd51660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7afb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1041cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a11d1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0fdbd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea52be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570cfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b3fbba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9fbfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e615d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9da0f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b3549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9cbaec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1e28a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80989985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c9d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025fb81a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf9f93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b73c06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ac460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a642ca54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b0ec1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cb69bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd85ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc5778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d062fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc05d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda70141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50769c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e149e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee4310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a9714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ff939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7c874d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516160ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f62c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc225afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337397ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9b9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17ab2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904de31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35084c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49012ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1f51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d375d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3267e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec2c347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3299d7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a77ca13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c81198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d175ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eecea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d96eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f8d152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d39e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b38a712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695dd964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b26dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ba9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a07fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff2e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
